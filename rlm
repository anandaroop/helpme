#! /usr/bin/env ruby

require 'dotenv'
require 'ruby_llm'

# find .env regardless of where this file is symlinked or run from

actual_path_to_this_file = (File.realpath(__FILE__))
this_dir = File.dirname(actual_path_to_this_file)
env_path = File.join(this_dir, '.env')
Dotenv.load(env_path)

# configure API keys

RubyLLM.configure do |config|
  config.anthropic_api_key = ENV['ANTHROPIC_API_KEY']
  config.openai_api_key = ENV['OPENAI_API_KEY']
end

# set up the chat completion

models = [
  "claude-3-7-sonnet-20250219",
  "gpt-4o-2024-05-13",
]

padding = models.map(&:length).max

chats = models.map do |model|
  RubyLLM.chat(model: model).with_temperature(0.1)
end

system_prompt = <<~SYSTEM
  You are an expert user of the following command line Unix utilities:
  - ffmpeg
  - imagemagick

  Your task is to take the user's request and produce the exact
  command line invocation that will accomplish the user's goal,
  including all necessary flags and arguments.

  Prefer ImageMagick over ffmpeg for image processing tasks.

  If you need more information from the user, state what's missing.

  Otherwise, no yapping, no markdown, no fluff. Just the command.
SYSTEM

chats.each do |chat|
  chat.add_message role: :system, content: system_prompt
end

# get the LLM response

user_request = ARGV.join(" ")

# anthropic_response = anthropic_chat.ask user_request
# anthropic_command = anthropic_response.content
# puts "[anthropic] #{anthropic_command}"

# openai_response = openai_chat.ask user_request
# openai_command = openai_response.content
# puts "[openai] #{openai_command}"

chats.each_with_index do |chat, i|
  response = chat.ask user_request
  command = response.content
  puts "%d [%#{padding}s] %s" % [i, chat.model.id, command]
end

# maybe run the command

# print "Run this command? [y/N]: "
# if STDIN.gets =~ /^y/i
#   system anthropic_command
# end
